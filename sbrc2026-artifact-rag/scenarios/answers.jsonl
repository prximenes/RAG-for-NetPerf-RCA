{"diagnosis": "CPU SoftIRQ saturation on a single core", "confidence": 1.0, "evidence": ["mpstat: CPU0 avg %soft ~87.0%", "/proc/interrupts: IRQ share skewed (CPU0=0.0%, CPU1=100.0%, CPU2=0.0%, CPU3=0.0%)", "ethtool -S: Δrx_dropped=9"], "next_steps": ["Rebalance IRQ affinity across CPUs (e.g., irqbalance or smp_affinity)", "Enable/tune RPS/XPS to distribute softirq", "Review NIC interrupt coalescing settings"], "scenario_id": "scenario_01", "scenario_name": "cpu_softirq_saturation"}
{"diagnosis": "TCP bufferbloat (queueing delay under saturation)", "confidence": 1.0, "evidence": ["ping: baseline avg 9.2ms -> saturated avg 441.3ms", "tc qdisc: qdisc fq_codel 1: dev eth0 root refcnt 2 limit 10240p flows 1024 quantum 1514 target 5.0ms interval 100.0ms ecn", "iperf3: avg throughput 93.3 Mbit/s"], "next_steps": ["Apply AQM (fq_codel/cake) and reduce buffer limits", "Traffic shape to below bottleneck rate"], "scenario_id": "scenario_02", "scenario_name": "tcp_bufferbloat"}
{"diagnosis": "Excessive context switches", "confidence": 1.0, "evidence": ["vmstat: avg cs/s ~122632"], "next_steps": ["Batch work; reduce per-request overhead", "Tune scheduler/affinity; reduce thread churn"], "scenario_id": "scenario_03", "scenario_name": "excessive_context_switches"}
{"diagnosis": "TCP listen queue overflows (small backlog)", "confidence": 1.0, "evidence": ["netstat -s: listen queue overflowed=789", "ss -ltn: max Recv-Q=128 max Send-Q=128", "sysctl: somaxconn=128 tcp_max_syn_backlog=4"], "next_steps": ["Increase net.core.somaxconn and tcp_max_syn_backlog", "Increase application listen backlog and tune accept loop"], "scenario_id": "scenario_04", "scenario_name": "tcp_listen_drops"}
{"diagnosis": "Path MTU mismatch causing fragmentation", "confidence": 1.0, "evidence": ["IP stats: reassemblies_required=1500 fragments_created=1800", "ping -M do: frag_needed=2 replies=0", "ip link: mtu=1400"], "next_steps": ["Align MTU across the path or enable PMTUD properly", "Avoid DF on oversize packets; adjust tunnel overhead"], "scenario_id": "scenario_05", "scenario_name": "mtu_fragmentation"}
{"diagnosis": "Small NIC ring buffers causing drops", "confidence": 1.0, "evidence": ["ethtool -G: RX=64 TX=512", "ethtool -S: Δrx_dropped=15000 Δrx_missed=0"], "next_steps": ["Increase RX/TX rings with ethtool -G", "Tune interrupt coalescing; verify driver limits"], "scenario_id": "scenario_06", "scenario_name": "small_ring_buffers"}
{"diagnosis": "TCP offloads disabled (TSO/GSO/GRO off)", "confidence": 1.0, "evidence": ["ethtool -k: generic-segmentation-offload=off, tcp-segmentation-offload=off, generic-receive-offload=off, large-receive-offload=off", "mpstat: avg %sys ~27.5%", "iperf3: avg throughput 980.0 Mbit/s"], "next_steps": ["Re-enable TSO/GSO/GRO where applicable", "Assess CPU utilization vs throughput trade-offs"], "scenario_id": "scenario_07", "scenario_name": "offloads_disabled"}
{"diagnosis": "High RTT and CPU usage due to emulated RTL8139 network driver overhead", "confidence": 1.0, "evidence": ["dmesg confirms usage of '8139too' driver for 'RealTek RTL8139' emulated device", "lspci identifies device as 'RTL-8139/8139C/8139C+' (QEMU Virtual Machine)", "ping statistics show high latency variance (mdev 15.122 ms) and max RTT of 89ms", "top shows high system CPU (%sy 65.2%) and significant kworker usage", "perf report indicates high overhead in 'native_write_msr' (VMExit/Hypervisor overhead) and 'rtl8139_start_xmit'", "ethtool confirms link speed limited to 100Mb/s (typical for this emulated NIC)"], "next_steps": ["Replace the emulated RTL8139 network interface with a paravirtualized VirtIO-Net driver", "Update VM configuration (Libvirt/QEMU) to use model='virtio'", "Ensure 'virtio_net' module is loaded in the guest kernel", "Verify performance improvement with reduced CPU usage and stable RTT"], "scenario_id": "scenario_08", "scenario_name": "kvm_rtl8139_high_rtt_virtualization"}
{"diagnosis": "Single-core CPU saturation due to network softirq processing (VirtIO single-queue)", "confidence": 1.0, "evidence": ["top shows ksoftirqd/0 consuming 82.1% CPU on CPU0", "mpstat confirms CPU0 is saturated with %soft (80.3%) while CPU1 is idle", "/proc/interrupts shows all 'virtio0-input.0' interrupts (15M+) hitting CPU0 only", "perf top shows high overhead in 'virtnet_poll' and 'net_rx_action', indicating heavy packet processing", "ethtool shows significant 'rx_dropped' (45,210), likely due to ring buffer overflow when CPU0 is saturated"], "next_steps": ["Enable VirtIO Multiqueue to distribute network interrupt processing across multiple CPUs", "Configure 'ethtool -L eth0 combined 2' (or match CPU count)", "Verify irqbalance is running or manually pin interrupts to different cores using /proc/irq/SMP_AFFINITY", "Consider increasing ring buffer size with 'ethtool -G'"], "scenario_id": "scenario_09", "scenario_name": "kvm_network_responder_overload"}
{"diagnosis": "Network packet drops due to Application CPU saturation (User Space)", "confidence": 1.0, "evidence": ["top shows 'java' processes consuming nearly 100% of all 4 CPUs (User space %us > 90%)", "Load average (12.45) is significantly higher than CPU count (4), indicating overload", "vmstat shows high run queue (r=12-15), confirming CPU contention", "ss -nlt shows Recv-Q (129) > Send-Q (128) backlog for port 8080", "netstat -s indicates packets pruned from receive queue due to socket buffer overrun", "rx_dropped in ethtool suggests the kernel cannot push packets to the application fast enough"], "next_steps": ["Scale out the application or increase CPU resources for the VM", "Profile the Java application to identify performance regressions", "Implement traffic shaping or rate limiting upstream to prevent overload"], "scenario_id": "scenario_10", "scenario_name": "kvm_cpu_responder_bottleneck"}
{"diagnosis": "High latency at low load due to Aggressive Interrupt Coalescing and/or C-States", "confidence": 0.95, "evidence": ["Ping latency significantly higher at 1s interval (~1ms) vs flood mode (~0.12ms)", "ethtool -c shows 'rx-usecs: 100', causing up to 100us delay before interrupt", "Adaptive RX is enabled, which may not scale down fast enough for sparse traffic", "Turbostat shows high package C6 state usage (85.40%), suggesting wake-up latency"], "next_steps": ["Disable adaptive-rx and reduce rx-usecs with 'ethtool -C eth0 adaptive-rx off rx-usecs 0' to test latency", "Disable deep C-states in BIOS or kernel (idle=poll) for latency sensitive workloads", "Consider using 'busy_poll' if available for ultra-low latency"], "scenario_id": "scenario_11", "scenario_name": "kvm_low_frequency_interrupt_overhead"}
{"diagnosis": "High Host CPU usage and overhead due to Emulated E1000 Driver (Trap-and-Emulate)", "confidence": 1.0, "evidence": ["lspci confirms 'Intel Corporation 82540EM' (e1000) emulated device", "top on host shows qemu-system-x86 consuming >100% CPU (single core saturated + overhead)", "perf report shows significant time in 'handle_io', 'kvm_userspace_exit', and 'e1000_mmio_write', indicating heavy VMExit costs", "vmstat shows high context switch rate (cs) in guest relative to CPU usage", "Ping RTT is relatively high (~4-5ms) for a local VM connection"], "next_steps": ["Switch to 'virtio-net' paravirtualized driver in VM configuration", "Ensure Guest OS has virtio drivers installed (standard in modern Linux)", "Restart VM and verify reduced Host CPU usage and improved latency"], "scenario_id": "scenario_12", "scenario_name": "kvm_e1000_emulated_driver_cpu_load"}
{"diagnosis": "Throughput limited by E1000 emulated driver overhead (CPU saturation)", "confidence": 1.0, "evidence": ["iperf3 shows throughput limited to ~1.18 Gbps, far below typical KVM virtio performance (10G+)", "ethtool confirms 'Speed: 1000Mb/s' and e1000 driver usage", "top shows Guest CPU0 is saturated with system time (95.5% sy), indicating heavy kernel processing", "perf report shows 'e1000_xmit_frame' and 'iowrite32' as top consumers, confirming driver overhead (PIO/MMIO costs)", "Single CPU core is fully utilized by the driver, creating a bottleneck"], "next_steps": ["Replace E1000 with VirtIO network driver (virtio-net-pci) to bypass emulation overhead", "Enable multiqueue if switching to VirtIO for further scaling", "Verify throughput increases to expected 10Gbps+ range"], "scenario_id": "scenario_13", "scenario_name": "kvm_e1000_overhead_dominance_network_load"}
{"diagnosis": "High network latency due to Docker Bridge and Netfilter/Iptables overhead", "confidence": 1.0, "evidence": ["perf report shows high kernel overhead in 'ipt_do_table' (18.12%) and 'nf_hook_slow' (12.45%)", "Presence of 'br_handle_frame' and 'br_nf_pre_routing' confirms packets are traversing the bridge software path", "High System CPU (%sy 35.2%) and SoftIRQ (%si 10.2%) on host", "Ping RTT is high (~13ms avg) for container-to-container or host-to-container communication", "Large number of veth interfaces (64+) suggests many rules/chains to traverse"], "next_steps": ["Bypass Docker Bridge/NAT by using '--net=host' for performance-critical containers", "Use Macvlan or IPVlan drivers to connect containers directly to physical network", "Consider SR-IOV if hardware supports it for near-native performance"], "scenario_id": "scenario_14", "scenario_name": "docker_bridge_high_rtt_network_load"}
{"diagnosis": "High RTT Jitter due to Host CPU Saturation (Noisy Neighbor)", "confidence": 1.0, "evidence": ["top shows 'stress-ng-cpu' processes consuming ~96% User CPU (%us)", "Load average (75.45) exceeds CPU count (64), indicating run queue saturation", "Ping RTT shows extreme jitter (min 0.45ms, max 85ms, mdev 22.45ms)", "pidstat shows high involuntary context switches (nvcswch/s ~450) for stress processes, indicating scheduler contention", "Network processing threads (ksoftirqd/dockerd) are likely starved of CPU time"], "next_steps": ["Apply CPU limits to containers using '--cpus' or '--cpu-shares' to prevent starvation", "Isolate critical workloads using '--cpuset-cpus' to dedicate cores", "Use 'nice' or 'ionice' to deprioritize background batch jobs"], "scenario_id": "scenario_15", "scenario_name": "docker_bridge_cpu_load_rtt_variation"}
{"diagnosis": "Packet loss due to Single Core XDP CPU Saturation", "confidence": 1.0, "evidence": ["mpstat shows CPU0 is 100% utilized by SoftIRQ (%soft), while other cores are idle", "ethtool shows massive 'rx_missed_errors' (7.78 Mpps), indicating the NIC ring is overflowing because the CPU cannot process packets fast enough", "perf top shows 65% of CPU time spent in 'bpf_prog_5a2_xdp_packet_processor', confirming XDP program cost is the bottleneck", "Throughput is capped at ~7.09 Mpps, well below the 14.88 Mpps line rate", "xdp_monitor shows no XDP_DROP, meaning packets are not being dropped by logic, but by hardware resource exhaustion"], "next_steps": ["Enable RSS (Receive Side Scaling) to distribute RX traffic across multiple CPUs", "Optimize the BPF program (bpf_prog_5a2) to reduce instruction count per packet", "Consider upgrading to a faster CPU or enabling JIT if not active (though perf shows it likely is)"], "scenario_id": "scenario_16", "scenario_name": "xdp_high_packet_loss_small_packets_cpu_load"}
{"diagnosis": "DPDK Application Throughput degradation due to CPU-intensive payload processing", "confidence": 1.0, "evidence": ["dpdk_app_stats.log shows RX Rings are full (Used 4096), causing 50% packet drop", "ethtool shows high 'rx_missed_errors' (7.5 Mpps), confirming NIC cannot DMA packets because rings are full", "top shows DPDK lcores (CPU0, CPU1) are 100% utilized in user space", "perf report indicates heavy CPU usage in 'deep_packet_inspection_regex' and 'decrypt_payload_aes_gcm'", "The bottleneck is the application logic, not the I/O"], "next_steps": ["Optimize application logic (DPI/Decryption) or use hardware accelerators (QAT)", "Scale out to more cores/queues to distribute the processing load", "Verify if encryption offload is supported by the NIC and enable it"], "scenario_id": "scenario_17", "scenario_name": "dpdk_throughput_degradation_cpu_intensive_processing"}
{"diagnosis": "Severe XDP throughput degradation due to inefficient/complex BPF program", "confidence": 1.0, "evidence": ["bpftool profile shows 'Duration: 450 ns/packet', which restricts theoretical max throughput to ~2.2 Mpps per core (1s / 450ns)", "mpstat shows CPU0 is 100% saturated", "ethtool shows massive 'rx_missed_errors', confirming CPU bottleneck", "Observed throughput (2.08 Mpps) aligns with the theoretical limit of the BPF program's execution time"], "next_steps": ["Optimize BPF program logic (reduce loop counts, simplify map lookups)", "Scale out to more cores using RSS/Multiqueue", "Offload processing to SmartNIC/FPGA if available"], "scenario_id": "scenario_18", "scenario_name": "xdp_severe_degradation_intensive_processing"}
{"diagnosis": "Throughput drop due to PCIe Link Degradation (Speed/Width Downgrade)", "confidence": 1.0, "evidence": ["lspci shows LnkSta (Link Status) is 'Speed 2.5GT/s (Downgraded), Width x1 (Downgraded)'", "LnkCap (Link Capability) supports '5GT/s, Width x8'", "dmesg explicitly warns 'PCI Express bandwidth... is not sufficient'", "ethtool shows high 'rx_missed_errors', indicating PCIe bus cannot drain the NIC buffer fast enough", "DPDK app shows low CPU usage (15%), confirming the bottleneck is I/O (PCIe), not CPU"], "next_steps": ["Reseat the network card in the PCIe slot", "Check BIOS/UEFI settings for PCIe configuration (Bifurcation, Speed caps)", "Replace the PCIe riser card or motherboard if physical damage is suspected"], "scenario_id": "scenario_19", "scenario_name": "dpdk_network_load_throughput_drop"}
{"diagnosis": "High Packet Loss due to intentional XDP DROP action (ACL/Firewall)", "confidence": 1.0, "evidence": ["xdp_monitor shows 'XDP_DROP: 12380000 pps', indicating the BPF program is explicitly executing XDP_DROP", "ethtool shows 'rx_missed_errors: 0', meaning the hardware and CPU are keeping up with the load", "mpstat shows CPU0 is not saturated (35% utilization), further confirming dropping is efficient and intentional", "High drop rate (83%) suggests a restrictive firewall policy or DDoS mitigation logic"], "next_steps": ["Review XDP program logic and maps (ACLs, blacklists) to verify if drops are expected", "Check for recent rule updates that might be too aggressive", "Analyze dropped packet headers if possible (using xdpdump or similar) to confirm valid traffic isn't being dropped"], "scenario_id": "scenario_20", "scenario_name": "xdp_extreme_packet_loss_intensive_processing"}
{"diagnosis": "VirtIO TX Ring Buffer Saturation", "confidence": 0.98, "evidence": ["ethtool -S: Significant increase in 'tx_dropped' (from 150 to 24580) during the capture window", "ethtool -G: Current TX ring size is small (256) compared to maximum supported (4096)", "ping: 12% packet loss correlated with high TX activity", "mpstat: High %sys usage (~45-50%) indicating kernel-side pressure processing packets"], "next_steps": ["Increase TX ring size using 'ethtool -G eth0 tx 1024' or higher", "Check host-side vhost-net threads for saturation", "Verify if packet generator application can use batching (sendmmsg)"], "scenario_id": "scenario_21", "scenario_name": "kvm_virtio_tx_queue_overflow"}
{"diagnosis": "CPU Steal Time / Noisy Neighbor", "confidence": 1.0, "evidence": ["top: High %st (steal time) of 37.0%", "mpstat: %steal consistently between 35-42%, indicating host CPU contention", "ping: High and variable RTT (jitter) ranging from 21ms to 215ms", "dmesg: Clocksource unstable/skew warning, common in VMs with erratic scheduling latency"], "next_steps": ["Migrate VM to a less loaded host", "Increase CPU shares/limit if using Cgroups/Docker", "Check for 'noisy neighbors' on the physical host", "Resize instance type if on public cloud (e.g., move from burstable T3 to dedicated C5)"], "scenario_id": "scenario_22", "scenario_name": "kvm_cpu_steal_time_latency"}
{"diagnosis": "SR-IOV Mac Spoofing Violation", "confidence": 1.0, "evidence": ["ip_addr_show_vm: VM interface eth0 has MAC 'aa:bb:cc:dd:ee:99'", "host_ip_link_show_pf: VF 1 (likely the one assigned) has assigned MAC 'aa:bb:cc:dd:ee:02' with 'spoofchk on'", "ping_gateway: Destination Host Unreachable (ARP failure)", "tcpdump_vm: ARP requests sent but no replies (packets dropped by switch/VF)", "ethtool_S_host_pf: 'rx_spoof_errors' count is non-zero (5432)"], "next_steps": ["Disable spoof checking on the host (ip link set dev <PF> vf <ID> spoofchk off) if MAC changes are required", "Configure the VM to use the assigned MAC address (aa:bb:cc:dd:ee:02)", "If using containers/macvlan inside VM, ensure 'trust' mode is on"], "scenario_id": "scenario_23", "scenario_name": "sriov_spoofchk_drops"}
{"diagnosis": "Netfilter Conntrack Table Exhaustion", "confidence": 1.0, "evidence": ["dmesg: Repeated 'nf_conntrack: table full, dropping packet' error messages", "sysctl: 'nf_conntrack_count' (65536) has reached 'nf_conntrack_max' (65536)", "slabtop: 'nf_conntrack' cache shows 65536 active objects with 100% usage", "curl_test: Intermittent 'Connection timed out' indicating packet drops for new flows"], "next_steps": ["Increase conntrack limit: 'sysctl -w net.netfilter.nf_conntrack_max=262144'", "Reduce TCP established timeout: 'sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=600' (default 5 days is often too high)", "Investigate workload for connection leaks or UDP floods filling the table"], "scenario_id": "scenario_24", "scenario_name": "conntrack_exhaustion"}
